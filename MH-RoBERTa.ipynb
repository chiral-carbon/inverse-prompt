{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=False):\n",
    "    \"\"\" Generate a word from out[gen_idx]\n",
    "\n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k\n",
    "    \"\"\"\n",
    "    logits = out[:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
    "    elif sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1)\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1)\n",
    "    return idx.tolist() if return_list else idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1024, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_text(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    text: a string of text with a question and answer\n",
    "    tokenizer: a HuggingFace tokenizer\n",
    "    model: a HuggingFace model\n",
    "\n",
    "    returns: a string of text generated from masking the question tokens and sampling from the model to fill in the masks\n",
    "    \"\"\"\n",
    "    init_tokens = tokenizer.tokenize(text)\n",
    "    question_mark_index = init_tokens.index('?')\n",
    "\n",
    "    for i in range(question_mark_index):\n",
    "        init_tokens[i] = tokenizer.mask_token\n",
    "    masked_init_text = tokenizer.convert_tokens_to_string(init_tokens)\n",
    "\n",
    "    input_ids = tokenizer(masked_init_text, return_tensors='pt')[\"input_ids\"]\n",
    "    output_logits = model(input_ids).logits\n",
    "    # print(F.softmax(output_logits, dim=-1))\n",
    "\n",
    "    pred_ids = input_ids.clone()[0].tolist()\n",
    "\n",
    "    for i in range(question_mark_index):\n",
    "        pred_ids[i+1] = generate_step(output_logits, i+1, sample=True)\n",
    "    \n",
    "    filled_toks = tokenizer.convert_ids_to_tokens(pred_ids[1:-1])\n",
    "    filled_text = tokenizer.convert_tokens_to_string(filled_toks)\n",
    "\n",
    "    return filled_toks, filled_text, question_mark_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Do the medicine people charge too much money for the special medicine that saves lives? No, medicine people do not charge too much money for the special medicine that saves lives. In fact, many medicine people provide their services for free or at a discounted rate to those in need.\n",
      "Infilled Text: Do that someone don questions tend those that medicine and health other and? medicine? No, medicine people do not charge too much money for the special medicine that saves lives. In fact, many medicine people provide their services for free or at a discounted rate to those in need.\n"
     ]
    }
   ],
   "source": [
    "text = \"Do the medicine people charge too much money for the special medicine that saves lives? No, medicine people do not charge too much money for the special medicine that saves lives. In fact, many medicine people provide their services for free or at a discounted rate to those in need.\"\n",
    "filled_toks, filled_text, question_mark_index = get_init_text(text, tokenizer, model)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Infilled Text:\", filled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_seq_log_probability(text):\n",
    "#     \"\"\"\n",
    "#     text: a string of text \n",
    "\n",
    "#     returns: the sum of the log probabilities of each token in the text\n",
    "#     \"\"\"\n",
    "\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     # prod = 1\n",
    "#     sum = 0\n",
    "\n",
    "#     for i, token in enumerate(tokens):\n",
    "#         masked_tokens = tokens.copy()\n",
    "#         masked_tokens[i] = tokenizer.mask_token\n",
    "#         masked_text = tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "#         # print(masked_text)\n",
    "#         inputs = tokenizer(masked_text, return_tensors=\"pt\")['input_ids'] \n",
    "#         # print(inputs)\n",
    "\n",
    "#         masked_index = torch.nonzero(inputs == tokenizer.mask_token_id)[0][-1]\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         logits = outputs.logits[0, masked_index.item(), :]\n",
    "#         probs = F.log_softmax(logits, dim=-1)\n",
    "#         token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "#         token_prob = probs[token_id]\n",
    "\n",
    "#         # prod *= token_prob.item()\n",
    "#         sum += token_prob.item()\n",
    "\n",
    "#     return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_prob = get_seq_log_probability(filled_text)\n",
    "# init_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gpt_logprobs(tokens):\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#       - tokens: tokens of the sequence\n",
    "#       - question_mark_index: index of '?' in the token list that separates the question and answer tokens\n",
    "\n",
    "#     Returns: \n",
    "#       - the sum of token logprobs from GPT\n",
    "#     \"\"\"\n",
    "#     # prefix = tokenizer.convert_tokens_to_string(tokens[:question_mark_index+1])\n",
    "#     # completion = tokenizer.convert_tokens_to_string(tokens[question_mark_index+1:])\n",
    "#     sequence = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "#     response = openai.Completion.create(model='text-davinci-003',\n",
    "#                                         prompt=sequence,\n",
    "#                                         max_tokens=0,\n",
    "#                                         logprobs=1,\n",
    "#                                         echo=True\n",
    "#                                         )\n",
    "#     # print(response.choices[0].logprobs.token_logprobs) \n",
    "#     return sum(filter(None, response.choices[0].logprobs.token_logprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"what is going on?\"\n",
    "\n",
    "# response = openai.Completion.create(model='gpt-3.5-turbo-instruct', prompt=prompt, max_tokens=10, logprobs=1)\n",
    "\n",
    "\n",
    "# # print(np.exp(sum(response.choices[0].logprobs.token_logprobs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\"/vast/work/public/ml-datasets/llama-2/Llama-2-7b-hf\")\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(\"/vast/work/public/ml-datasets/llama-2/Llama-2-7b-hf\")\n",
    "\n",
    "# prompt = \"Hey what's up? What happened to the case?\"\n",
    "\n",
    "# encoded = tokenizer_llama(prompt, return_tensors=\"pt\")\n",
    "# input_ids = [\"input_ids\"]\n",
    "\n",
    "# output = model_llama(input_ids=input_ids, max_length=10, do_sample=True, temperature=1.0, output_scores=True)\n",
    "\n",
    "# neglecting the first token, since we make no prediction about it\n",
    "# output_llama = model_llama.generate(input_ids, max_length=100, do_sample=True, top_k=50, top_p=0.95, temperature=1.0, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "# # transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "# output_length = inputs.input_ids.shape[1] + np.sum(transition_scores.numpy() < 0, axis=1)\n",
    "# # length_penalty = model.generation_config.length_penalty\n",
    "\n",
    "# probabilities = torch.exp(transition_scores.sum(axis=1))\n",
    "\n",
    "# print(tokenizer.batch_decode(output_llama.sequences[0]))\n",
    "# print(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobs_from_prompt(prompt, tokenizer, model):\n",
    "      encoded = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "      input_ids = encoded[\"input_ids\"]\n",
    "      output = model(input_ids=input_ids, max_tokens=1)\n",
    "      shift_labels = input_ids[..., 1:].contiguous()\n",
    "      shift_logits = output.logits[..., :-1, :].contiguous()\n",
    "\n",
    "      log_probs = []\n",
    "      log_probs.append((tokenizer.decode(input_ids[0].tolist()[0]), None))\n",
    "      for idx, (label_id, logit) in enumerate(zip(shift_labels[0].tolist(), shift_logits[0])):\n",
    "            logprob = F.log_softmax(logit, dim=0).tolist()[label_id]\n",
    "            log_probs.append((tokenizer.decode(label_id), float(logprob)))\n",
    "      return sum(log_probs)\n",
    "\n",
    "print(logprobs_from_prompt(prompt, tokenizer_llama, model_llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filled_toks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# delta\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# max_patience\u001b[39;00m\n\u001b[1;32m     11\u001b[0m accepted \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m current_tokens \u001b[38;5;241m=\u001b[39m filled_toks\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     14\u001b[0m current_seq \u001b[38;5;241m=\u001b[39m filled_text\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filled_toks' is not defined"
     ]
    }
   ],
   "source": [
    "# MH algorithm\n",
    "import numpy as np\n",
    "\n",
    "iter = 10\n",
    "top_k = 5\n",
    "sample = True\n",
    "\n",
    "# delta\n",
    "# max_patience\n",
    "\n",
    "accepted = 0\n",
    "\n",
    "current_tokens = filled_toks.copy()\n",
    "current_seq = filled_text\n",
    "\n",
    "for i in range(iter):\n",
    "    print(\"Iteration:\", i)\n",
    "    print(\"Current Sequence: \", current_seq)\n",
    "    curr_seq_energy = logprobs_from_prompt(current_seq, tokenizer_llama, model_llama)\n",
    "\n",
    "    for t in range(question_mark_index):\n",
    "        old_token = current_tokens[t]\n",
    "        current_tokens[t] = tokenizer.mask_token\n",
    "        \n",
    "        ids = tokenizer(tokenizer.convert_tokens_to_string(current_tokens), return_tensors='pt')[\"input_ids\"]\n",
    "        logits = model(ids).logits\n",
    "        probs = logits[0, t+1, :].softmax(dim=0)\n",
    "\n",
    "        old_token_id = tokenizer.convert_tokens_to_ids(old_token)\n",
    "        new_token_id = generate_step(logits, t+1, temperature=1.0, sample=sample).item()\n",
    "        print(f\"Old token: {old_token}, New token: {tokenizer.convert_ids_to_tokens(new_token_id)}\")\n",
    "        old_token_prob = probs[old_token_id].item()\n",
    "        new_token_prob = probs[new_token_id].item()\n",
    "\n",
    "        ids[0][t+1] = new_token_id\n",
    "\n",
    "        proposal_tokens = tokenizer.convert_ids_to_tokens(ids[0].tolist()[1:-1])\n",
    "        proposal_seq = tokenizer.convert_tokens_to_string(proposal_tokens)\n",
    "        proposal_seq_energy = logprobs_from_prompt(proposal_seq, tokenizer_llama, model_llama)\n",
    "\n",
    "        print(f\"Curr seq logprobs: {curr_seq_energy}, Proposal seq logprobs: {proposal_seq_energy}, New token prob: {new_token_prob}, Old token prob: {old_token_prob}\")\n",
    "\n",
    "        u = np.random.uniform(0, 1)\n",
    "        alpha = min(1, (np.exp(proposal_seq_energy - curr_seq_energy) * (old_token_prob/new_token_prob)))\n",
    "        if u <= alpha:\n",
    "            current_seq = proposal_seq\n",
    "            current_tokens = proposal_tokens.copy()\n",
    "            curr_seq_energy = proposal_seq_energy\n",
    "            accepted += 1\n",
    "    print(\"Proposal Sequence: \", proposal_seq)\n",
    "\n",
    "print(\"Final proposed sequence: \", proposal_seq)\n",
    "print(\"Acceptance rate:\", accepted/(iter*question_mark_index)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
